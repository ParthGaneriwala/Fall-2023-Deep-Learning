{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4744962-cf2d-414d-acf8-38e56a8c5bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MTH 4320 / 5320 - Homework 1\n",
    "\n",
    "## Gradient Descent, Linear Models, and Logistic Classification\n",
    "\n",
    "**Deadline**: Sept 15\n",
    "\n",
    "**Points**: TBA\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Submit **one** Python notebook file for grading. Your file must include **text explanations** of your work, **well-commented code**, and the **outputs** from your code.\n",
    "\n",
    "### Problems\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "1. Write a version of gradient descent with an option to use the explicit gradient formula of your loss function OR the `estimate_gradient` approximator.\n",
    "\n",
    "2. Write a version of gradient descent that chooses $n$ random starting points and outputs the parameters resulting in minimum training loss across all the runs.\n",
    "\n",
    "3. Write a version with a **cyclic** learning rate that changes in each training epoch and saves the model parameters every time the learning rate vanishes. Then, select the best parameters observed.\n",
    "\n",
    "4. Write functionality that makes predictions by all models with saved weights  by parts 2-3 and averages the output predictions of all the models.\n",
    "\n",
    "5. [Apply it to a regression dataset to be provided soon.]\n",
    "\n",
    "\n",
    "#### Logistic Classification\n",
    "\n",
    "6. Write a multi-class version of the BinaryLogisticClassifier in the scikit-learn class style (as in class) optimized by gradient descent.\n",
    "\n",
    "7. Derive a formula for the gradient of the MSE loss function with respect to the weights of your multi-class logistic classifier.\n",
    "\n",
    "8. Add functionality to optimize models using the three variants of gradient descent from Problems 2-4.\n",
    "\n",
    "9. Apply your classifier and all six variants of gradient descent (3 variants with estimated gradient and 3 variants with exact gradient) to classify the MNIST dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
