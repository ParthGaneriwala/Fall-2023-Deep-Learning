{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4744962-cf2d-414d-acf8-38e56a8c5bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MTH 4320 / 5320 - Homework 1\n",
    "\n",
    "## Gradient Descent, Linear Models, and Logistic Classification\n",
    "\n",
    "**Deadline**: Sept 17\n",
    "\n",
    "**Points**: 100\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Submit **one** Python notebook file for grading. Your file must include **text explanations** of your work, **well-commented code**, and the **outputs** from your code.\n",
    "\n",
    "### Problems\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "1. [10 points] Write a version of gradient descent with an option to use the explicit gradient formula of your loss function OR the `estimate_gradient` approximator.\n",
    "\n",
    "2. [10 points] Write a version of gradient descent that chooses $n$ random starting points and outputs the parameters resulting in minimum training loss across all the runs.\n",
    "\n",
    "3. [10 points] Write a version with a **cyclic** learning rate that changes in each training epoch and saves the model parameters every time the learning rate vanishes. Then, select the best parameters observed.\n",
    "\n",
    "4. [10 points] For the gradient descent variants from problems 2-3, instead of saving only the best parameters, save all sets of parameters that are converged to, predict outputs using each set of parameters, and then have final output predictions equal to the mean of the model predictions.\n",
    "\n",
    "9. [15 points] Apply your model with all six variants of gradient descent (3 variants with estimated gradient and 3 variants with exact gradient) to predict house prices in the [Mount Pleasant Real Estate Dataset](https://www.hawkeslearning.com/Statistics/dis/datasets.html) using columns C-O, Q-T, and V-W (note some columns will need to be converted to binary or one-hot representations). Compare the results in terms of test accuracy, `fit` runtime, and `predict` runtime. \n",
    "\n",
    "#### Logistic Classification\n",
    "\n",
    "6. [10 points] Write a multi-class version of the `BinaryLogisticClassifier` from lecture in the style of `scikit-learn` classifiers (i.e. as a Python class with `fit` and `predict` functions), optimized by gradient descent.\n",
    "\n",
    "7. [10 points] Derive a formula for the gradient of the MSE loss function with respect to the weights of your multi-class logistic classifier.\n",
    "\n",
    "8. [10 points] Add functionality to optimize models using the three variants of gradient descent from Problems 2-4.\n",
    "\n",
    "9. [15 points] Apply your classifier and all six variants of gradient descent (3 variants with estimated gradient and 3 variants with exact gradient) to classify the MNIST dataset. Compare the results in terms of test accuracy, `fit` runtime, and `predict` runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
